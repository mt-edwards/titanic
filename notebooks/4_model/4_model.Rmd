---
title: "Model"
author: "Matthew Edwards"
date: "30/09/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(here)
```

## Read data

```{r}
titanic_train <- read_rds(here("data", "titanic_train.rds"))
titanic_val <- read_rds(here("data", "titanic_val.rds"))
titanic_test <- read_rds(here("data", "titanic_test.rds"))
```

## Preprocess data

```{r}
rec <- titanic_train %>%
  recipe(survived ~ .) %>%
  step_other(title, deck) %>% 
  prep(training = titanic_train)

titanic_train <- bake(rec, new_data = titanic_train)
titanic_val <- bake(rec, new_data = titanic_val)
titanic_test <- bake(rec, new_data = titanic_test)
```

## Model specification

### Hyper-parameter tuning

The random forest model (and other machine learning models) have parameters which can be used to control overfitting and hence create a more accurate predictive model. Hyper-parameter tuning can be done manually, but since we already have a metric to maximise (the f1-measure), it can be done autmatically. Firstly, select a random set of hyper-parameters and train the model on the training set then evaluate the models f1-measure using the validation set.

There are many methods we could use to explore the hyper-parameters, however a simple random grid proves surprisingly effective. 

```{r}
n_models <- 100

param_grid <- grid_random(
  mtry %>% range_set(c(1,  5)),
  trees %>% range_set(c(500, 1000)), 
  min_n %>% range_set(c(2,  10)),
  size = n_models
)

models <- rand_forest(mode = "classification") %>%
  set_engine("ranger") %>% 
  merge(param_grid)
```

## Model fitting

`map` is used to apply the function `fit` to each of the random forest model specifications generated using the random-grid of hyper-parameters. The same training data (`titanic_train`) and formula is used for each model - so only the values of the hyper-parameters can influence the model fit.

```{r}
map(models, fit, formula = survived ~ ., data = titanic_train)
```

# Model validation

Each model is used to perform a prediction on the validation set (`titanic_val`) and the `f_measure` is calculated for each model using `map_dfr` which creates a dataframe by rowbinding the results of each application of `f_meas`. Finally, the results are labelled using `row_number()` which allows us to extract the index of the best performing model, which is the model with the highest f1-measure.

```{r}
f_measures <- map(fits, predict, new_data = titanic_val) %>% 
  map(bind_cols, titanic_val) %>% 
  map_dfr(f_meas, truth = survived, estimate = .pred_class) %>% 
  transmute(id = row_number(), f_measure = .estimate) %>% 
  arrange(desc(f_measure))
```

Select the best performing model from the `r n_models` considered.

```{r}
best_model <- f_measures[which.max(f_measures$f_measure),]
```


## Testing

Now, look at the performance of the selected model using the test data (`titanic_test`).

```{r}
predict(nth(fits, best_model$id), new_data = titanic_test) %>%
  bind_cols(titanic_test) %>%
  metric_set(accuracy, f_meas)(truth = survived, estimate = .pred_class)
```
